NVDA earnings call for the period ending December 31, 2024.IMAGE SOURCE: THE MOTLEY FOOL.Nvidia (NVDA -0.04%)Q4 2025 Earnings CallFeb 26, 2025, 5:00 p.m. ETContents:* Prepared Remarks* Questions and Answers* Call ParticipantsPrepared Remarks:OperatorGood afternoon. My name is Krista, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's fourth-quarter earnings call. All lines have been placed on mute to prevent any background noise.After the speakers' remarks, there will be a question-and-answer session. [Operator instructions] Thank you. Stewart Stecker, you may begin your conference.Stewart Stecker -- Senior Director, Investor RelationsThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectations.These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, February 26, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.During this call, we will discuss non-GAAP financial measures. Confine a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentThanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion was up 12% sequentially and up 78% year on year and above our outlook of $37.5 billion. For fiscal 2025 revenue was $130.5 billion, up 114% from the prior year.Let's start with Data Center. Data center revenue for fiscal 2025 was $115.2 billion, more than doubling from the prior year. In the fourth quarter, Data Center revenue of $35.6 billion was a record, up 16% sequentially and 93% year on year as the Blackwell ramp commenced and Hopper 200 continued sequential growth. In Q4, Blackwell sales exceeded our expectations.We delivered $11 billion of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale. Blackwell production is in full-year across multiple configurations, and we are increasing supply quickly expanding customer adoption. Our Q4 Data Center compute revenue jumped 18% sequentially and over 2x year on year.Customers are racing to scale infrastructure to train the next generation of cutting-edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size. Post-training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine-tuning reinforcement learning and distillation to tailor models for domain-specific use cases.Hugging Face alone hosts over 90,000 derivatives freighted from the Llama foundation model. The scale of post-training and model customization is massive and can collectively demand orders of magnitude, more compute than pretraining. Our inference demand is accelerating, driven by test time scaling and new reasoning models like OpenAI's o3, DeepSeek-R1, and Grok 3. Long-thinking reasoning AI can require 100x more compute per task compared to one-shot inferences.Blackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25x higher token throughput and 20x lower cost versus Hopper 100. It is revolutionary transformer engine is built for LLM and mixture of experts inference. And its NVLink Domain delivers 14x the throughput of PCIe Gen 5, ensuring the response time, throughput, and cost efficiency needed to tackle the growing complexity of infants of scale.Companies across industries are tapping into NVIDIA's whole STAG inference platform to boost performance and slash costs. Now, tripled inference throughput and cut costs by 66%, using NVIDIA TensorRT for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3x with NVIDIA Triton Inference Server and TensorRT-LLM. Microsoft Bing achieved a 5x speed up at major TCO savings for visual search across billions of images with NVIDIA, TensorRT, and acceleration libraries.Blackwell has great demand for inference. Many of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pretraining, post-training to inference across cloud, to on-premise, to enterprise. CUDA's programmable architecture accelerates every AI model and over 4,400 applications, ensuring large infrastructure investments against obsolescence in rapidly evolving markets.Our performance and pace of innovation is unmatched. We're driven to a 200x reduction in inference costs in just the last two years. We delivered the lowest TCO and the highest ROI. And full stack optimizations for NVIDIA and our large ecosystem, including 5.9 million developers continuously improve our customers' economics.In Q4, large CSPs represented about half of our data center revenue. and these sales increased nearly 2x year on year. Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS, and OCI bringing GB200 systems to cloud regions around the world to meet certain surging customer demand for AI. Regional cloud hosting NVIDIA GPUs increased as a percentage of data center revenue, reflecting continued AI factory build-outs globally and rapidly rising demand for AI reasoning models and agents.We've launched a 100,000 GB200 cluster-based incidents with NVLink Switch and Quantum 2 InfiniBand. Consumer Internet revenue grew 3x year on year, driven by an expanding set of generative AI and deep learning use cases. These include recommender systems, vision, language understanding, synthetic data generation search, and agentic AI. For example, xAI is adopting the GB200 to train and inference its next generation of Grok AI models.Meta's cutting-edge Andromeda advertising engine runs on NVIDIA's Grace Hopper Superchip serving vast quantities of ads across Instagram, Facebook applications. Andromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by 3x, enhance ad personalization, and deliver meaningful jumps in monetization and ROI. Enterprise revenue increased nearly 2x year on accelerating demand for model fine-tuning, RAG, and agentic AI workflows, and GPU-accelerated data processing. We introduced NVIDIA Llama Nemotron model family NIMs to help developers create and deploy AI agents across a range of applications including customer support, fraud detection, and product supply chain and inventory management.Leading AI agent platform providers, including SAP and ServiceNow, are among the first to use new models. Health care leaders IQVIA, Illumina, Mayo Clinic, and Arc Institute are using NVIDIA AI to speed drug discovery enhanced genomic research and Pioneer advanced healthcare services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications and autonomous vehicles where virtually, every AV company is developing on NVIDIA in the data center, the car, or both.NVIDIA's automotive vertical revenue is expected to grow to approximately $5 billion this fiscal year. At CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development and smart factory initiatives. Vision transformers, self-supervised learning, multimodal sensor fusion, and high-fidelity simulation are driving breakthroughs in AV development and will require 10x more compute. At CES, we announced the NVIDIA COSMO World Foundation model platform.Just as language, foundation models have revolutionized Language AI, Cosmos is a physical AI to revolutionize robotics. The robotics and automotive companies, including ridesharing giant Uber, are among the first to adopt the platform. From a geographic perspective, sequential growth in our Data Center revenue was strongest in the U.S., driven by the initial ramp up Blackwell. Countries across the globe are building their AI ecosystem as demand for compute infrastructure is surging.France's EUR 100 billion AI investment and the EU's EUR 200 billion invest AI initiatives offer a glimpse into the build-out to set redefined global AI infrastructure in the coming years. Now, as a percentage of total Data Center revenue, data center sales in China remained well below levels seen on the onset of export controls. Absent any change in regulations, we believe that China shipments will remain roughly at the current percentage. The market in China for data center solutions remains very competitive.We will continue to comply with export controls while serving our customers. Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from small NVLink 8 with InfiniBand, to large NVLink 72 with Spectrum-X.Spectrum-X and NVLink Switch revenue increased and represents a major new growth vector. We expect networking to return to growth in Q1. AI requires a new class of networking. NVIDIA offers NVLink Switch systems for scale-up compute.For scale-out, we offer quantum incentive for HPC supercomputers and Spectrum X for Ethernet environments. Spectrum-X enhances the Ethernet for AI computing and has been a huge success. Microsoft Azure, OCI, CoreWeave, and others are building large AI factories with Spectrum-X. The first Stargate data centers will use Spectrum-X.Yesterday, Cisco announced integrating Spectrum-X into their networking portfolio to help enterprises build AI infrastructure. With its large enterprise footprint and global reach, Cisco will bring NVIDIA Ethernet to every industry. Now, moving to gaming and ARPCs. Gaming revenue of $2.5 billion decreased 22% sequentially and 11% year on year.Full-year revenue of $11.4 billion increased 9% year on year, and demand remains strong throughout the holiday. However, Q4 shipments were impacted by supply constraints. We expect strong sequential growth in Q1 as supply increases. The new GeForce RTX 50 Series desktop and laptop GPUs are here.Build for gamers, creators, and developers they fuse AI and graphics redefining visual computing, powered by the Blackwell architecture, fifth-generation Tensor cores, and fourth-generation RT cores and featuring UQ's400AI tops. These GPUs deliver a 2x performance leap and new AI-driven rendering including neuro shaders, digital human technologies, geometry, and lighting. The new DLSS 4 boost frame rates up to 8x with AI-driven frame generation, turning one rendered frame into three. It also features the industry's first real-time application of transformer models packing 2x more parameters and 4x to compute for unprecedented visual fidelity.We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA Max-Q technology that extends battery life by up to an incredible 40%. These laptops will be available starting in March from the world's top manufacturers. Moving to our professional visualization business. Revenue of $511 million was up 5% sequentially and 10% year on year.Full-year revenue of $1.9 billion increased 21% year on year. Key industry verticals driving demand include automotive and healthcare. NVIDIA Technologies and generative AI are reshaping design, engineering, and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms from ANSYS, Cadence, and Siemens fueling demand for NVIDIA RTX workstations.Now, moving to Automotive. Revenue was a record $570 million, up 27% sequentially and up 103% year on year. Full-year revenue of $1.7 billion increased 5% year on year. Strong growth was driven by the continued ramp in autonomous vehicles, including cars and robotaxis.At CES, we announced Toyota, the world's largest automaker will build its next-generation vehicles on NVIDIA Orin running the safety-certified NVIDIA DriveOS. We announced Aurora and Continental will deploy driverless trucks at scale powered by NVIDIA Drive Thor. Finally, our end-to-end autonomous vehicle platform NVIDIA Drive Hyperion has passed industry safety assessments like TUV SUD and TUV Rheinland, two of the industry's foremost authorities for automotive-grade safety and cybersecurity. NVIDIA is the first AV platform that received a comprehensive set of third-party assessments.OK. Moving to the rest of the P&L. GAAP gross margin was 73% and non-GAAP gross margin was 73.5% and down sequentially as expected with our first deliveries of the Blackwell architecture. As discussed last quarter, Blackwell is a customizable AI infrastructure with several different types of NVIDIA build chips multiple networking options, and for air and liquid-cooled data center.We exceeded our expectations in Q4 in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect gross margins to be in the low 70s. Initially, we are focused on expediting the manufacturing of Blackwell systems to meet strong customer demand as they race to build out Blackwell infrastructure. When fully ramped, we have many opportunities to improve the cost, and gross margin will improve and return to the mid-70s, late this fiscal year.Sequentially, GAAP operating expenses were up 9% and non-GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and infrastructure costs for new product introductions. In Q4, we returned $8.1 billion to shareholders in the form of share repurchases and cash dividends. Let me turn to the outlook in the first quarter. Total revenue is expected to be $43 billion, plus or minus 2%.Continuing with its strong demand, we expect a significant ramp of Blackwell in Q1. We expect sequential growth in both Data Center and Gaming. Within Data Center, we expect sequential growth from both compute and networking. GAAP and non-GAAP gross margins are expected to be 70.6% and 71%, respectively, plus or minus 50 basis points.GAAP and non-GAAP operating expenses are expected to be approximately $5.2 billion and $3.6 billion, respectively. We expect full-year fiscal year '26 operating expenses to grow to be in the mid-30s. GAAP and non-GAAP other incoming expenses are expected to be an income of approximately $400 million. excluding gains and losses from nonmarketable and publicly held equity securities.GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. In closing, let me highlight upcoming events for the financial community. We will be at the TD Cowen Health Care Conference in Boston on March 3 and at the Morgan Stanley Technology, Media, and Telecom Conference in San Francisco on March 5.Please join us for our Annual GTC conference starting Monday, March 17 in San Jose, California. Jensen will deliver a news-packed keynote on March 18, and we will host a Q&A session for our financial analysts for the next day, March 19. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal 2026 is scheduled for May 28, 2025.We are going to open up the call, operator, to questions. If you could start that, that would be great.Questions & Answers:OperatorThank you. [Operator instructions] And your first question comes from C.J. Muse with Cantor Fitzgerald. Please go ahead.C.J. Muse -- AnalystYeah. Good afternoon. Thank you for taking the question. I guess for me, Jensen, as test-time compute and reinforcement learning shows such promise, we're clearly seeing an increasing blurring of the lines between training and inference.What does this mean for the potential future of potentially inference dedicated clusters? And how do you think about the overall impact to NVIDIA and your customers? Thank you.Jensen Huang -- President and Chief Executive OfficerYeah, I appreciate that, C.J. There are now multiple scaling laws. There's the pre-training scaling law, and that's going to continue to scale because we have multimodality, we have data that came from reasoning that are now used to do pretraining. And then the second is post-training skilling, using reinforcement learning human feedback, reinforcement learning AI feedback, reinforcement learning, verifiable rewards.The amount of computation you use for post-training is actually higher than pretraining. And it's kind of sensible in the sense that you could, while you're using reinforcement learning, generate an enormous amount of synthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. And that's post-trade.And the third part, this is the part that you mentioned is test-time compute or reasoning, long thinking, inference scaling. They're all basically the same ideas. And there, you have a chain of thought, you have search. The amount of tokens generated the amount of inference compute needed is already 100x more than the one-shot examples and the one-shot capabilities of large language models in the beginning.And that's just the beginning. This is just the beginning. The idea that the next generation could have thousands of times and even, hopefully, extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions of times more compute than today is in our future. And so, the question is, how do you design such an architecture? Some of it -- some of the models are auto-regressive.Some of the models are diffusion-based. Some of it -- some of the times you want your data center to have disaggregated inference. Sometimes, it is compacted. And so, it's hard to figure out what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular.We run every model. We are great at training. The vast majority of our compute today is actually inference and Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind.And when you look at training, it's many times more performing. But what's really amazing is for long thinking test time scaling, reasoning AI models were tens of times faster, 25x higher throughput. And so, Blackwell is going to be incredible across the board. And when you have a data center that allows you to configure and use your data center based on are you doing more pretraining now, post-training now, or scaling out your inference, our architecture is fungible and easy to use in all of those different ways.And so, we're seeing, in fact, much, much more concentration of a unified architecture than ever before.OperatorYour next question comes from the line of Joe Moore with JPMorgan. Please go ahead.Joe Moore -- JPMorgan Chase and Company -- AnalystMorgan Stanley, actually. Thank you. I wonder if you could talk about GB200 at CES, you sort of talked about the complexity of the rack-level systems and the challenges you have. And then as you said in the prepared remarks, we've seen a lot of general availability.Where are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level? And just have you maintained your enthusiasm for the NVL72 platforms?Jensen Huang -- President and Chief Executive OfficerWell, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more since CES. We have some 350 plants manufacturing the 1.5 million components that go into each one of the Blackwell racks, Grace Blackwell racks. Yes, it's extremely complicated.And we successfully and incredibly ramped up Grace Blackwell, delivering some $11 billion of revenues last quarter. We're going to have to continue to scale as demand is quite high, and customers are anxious and impatient to get their Blackwell systems. You've probably seen on the web, a fair number of celebrations about Grace Blackwell systems coming online and we have them, of course. We have a fairly large installation of Grace Blackwell for our own engineering and our own design teams and software teams.CoreWeave has now been quite public about the successful bring-up of theirs. Microsoft has, of course, OpenAI has, and you're starting to see many come online. And so, I think the answer to your question is nothing is easy about what we're doing, but we're doing great, and all of our partners are doing great.OperatorYour next question comes from the line of Vivek Arya with Bank of America Securities. Please go ahead.Vivek Arya -- AnalystThank you for taking my question. Colette if you wouldn't mind confirming if Q1 is the bottom for gross margins? And then Jensen, my question is for you. What is on your dashboard to give you the confidence that the strong demand can sustain into next year? And has DeepSeek and whatever innovations they came up with, has that changed that view in any way? Thank you.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentWell, let me first take the first part of the question there regarding the gross margin. During our Blackwell ramp, our gross margins will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide to customers as soon as possible. Our Blackwell is fully round.And once it does -- I'm sorry, once our Blackwell fully rounds, we can improve our cost and our gross margin. So, we expect to probably be in the mid-70s later this year. Walking through what you heard Jensen speak about the systems and their complexity, they are customizable in some cases. They've got multiple networking options.They have liquid-cooled and water-cooled. So, we know there is an opportunity for us to improve these gross margins going forward. But right now, we are going to focus on getting the manufacturing complete and to our customers as soon as possible.Jensen Huang -- President and Chief Executive OfficerWe know several things, Vivek. We have a fairly good line of sight of the amount of capital investment that data centers are building out toward. We know that going forward, the vast majority of software is going to be based on machine learning. And so, accelerated computing and generative AI, reasoning AI are going to be the type of architecture you want in your data center.We have, of course, forecasts and plans from our top partners. And we also know that there are many innovative, really exciting start-ups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs, reasoning AI, or physical AIs. The number of start-ups are still quite vibrant and each one of them needs a fair amount of computing infrastructure. And so, I think the -- whether it's the near-term signals or the midterm signals, near-term signals, of course, are POs and forecasts and things like that.Midterm signals would be the level of infrastructure and capex scale-out compared to previous years. And then the long-term signals has to do with the fact that we know fundamentally software has changed from hand-coding that runs on CPUs to machine learning and AI-based software that runs on GPUs and accelerated computing systems. And so, we have a fairly good sense that this is the future of software. And then maybe as you roll it out, another way to think about that is we've really only tapped consumer AI and search and some amount of consumer generative AI, advertising, recommenders, kind of the early days of software.The next wave is coming, agentic AI for enterprise, physical AI for robotics, and sovereign AI as different regions build out their AI for their own ecosystems. And so, each one of these are barely off the ground, and we can see them. We can see them because, obviously, we're in the center of much of this development and we can see great activity happening in all these different places and these will happen. So, near term, midterm, long term.OperatorYour next question comes from the line of Harlan Sur with JPMorgan. Please go ahead.Harlan Sur -- AnalystYeah. Good afternoon. Thanks for taking my question. Your next-generation Blackwell Ultra is set to launch in the second half of this year, in line with the team's annual product cadence.Jensen, can you help us understand the demand dynamics for Ultra given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products? And is the team still on track to execute Blackwell Ultra in the second half of this year?Jensen Huang -- President and Chief Executive OfficerYes. Blackwell Ultra is second half. As you know, the first Blackwell was we had a hiccup that probably cost us a couple of months. We're fully recovered, of course.The team did an amazing job recovering and all of our supply chain partners and just so many people helped us recover at the speed of light. And so, now we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories, and of course, new processors, and all of that is coming online.We have been working with all of our partners and customers, laying this out. They have all of the necessary information, and we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from Hopper to Blackwell because we went from an NVLink 8 system to an NVLink 72-based system.So, the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition. But the next transition will slot right in Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that.And the click after that is called Vera Rubin and all of our partners are getting up to speed on the transition of that and so preparing for that transition. And again, we're going to provide a big, huge step-up. And so, come to GTC, and I'll talk to you about Blackwell Ultra, Vera Rubin and then show you what we place after that. Really exciting new products to come to GTC piece.OperatorYour next question comes from the line of Timothy Arcuri with UBS. Please go ahead.Timothy Arcuri -- AnalystThanks a lot. Jensen, we heard a lot about custom ASICs. Can you kind of speak to the balance between customer ASIC and merchant GPU? We hear about some of these heterogeneous superclusters to use both GPU and ASIC. Is that something customers are planning on building? Or will these infrastructures remain fairly distinct? Thanks.Jensen Huang -- President and Chief Executive OfficerWell, we built very different things than ASICs, in some ways, completely different in some areas we intercept. We're different in several ways. One, NVIDIA'S architecture is general whether you're -- you've optimized for unaggressive models or diffusion-based models or vision-based models or multimodal models, or text models. We're great in all of it.We're great on all of it because our software stack is so -- our architecture is sensible. Our software stack ecosystem is so rich that we're the initial target of most exciting innovations and algorithms. And so, by definition, we're much, much more general than narrow. We're also really good from the end-to-end from data processing, the curation of the training data, to the training of the data, of course, to reinforcement learning used in post-training, all the way to inference with tough time scaling.So, we're general, we're end-to-end, and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem. We could be in a robot. Our architecture is much more accessible and a great target initial target for anybody who's starting up a new company.And so, we're everywhere. And the third thing I would say is that our performance in our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixing power.And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues. And so, if you have a 100-megawatt data center, if the performance or the throughput in that 100-megawatt or the gigawatt data center is four times or eight times higher, your revenues for that gigawatt data center is eight times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so, the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROI.And so, I think the third reason is performance. And then the last thing that I would say is the software stack is incredibly hard. Building an ASIC is no different than what we do. We build a new architecture.And the ecosystem that sits on top of our architecture is 10 times more complex today than it was two years ago. And that's fairly obvious because the amount of software that the world is building on top of architecture is growing exponentially and AI is advancing very quickly. So, bringing that whole ecosystem on top of multiple chips is hard. And so, I would say that -- those four reasons.And then finally, I will say this, just because the chip is designed doesn't mean it gets deployed. And you've seen this over and over again. There are a lot of chips that get built, but when the time comes, a business decision has to be made, and that business decision is about deploying a new engine, a new processor into a limited AI factory in size, in power, and in fine. And our technology is not only more advanced, more performance, it has much, much better software capability and very importantly, our ability to deploy is lightning fast.And so, these things are enough for the faint of heart, as everybody knows now. And so, there's a lot of different reasons why we do well, why we win.OperatorYour next question comes from the line of Ben Reitzes with Melius Research. Please go ahead.Ben Reitzes -- AnalystYeah. Hi. Ben Reitzes here. Hey, thanks a lot for the question.Jensen, it's a geography-related question. You did a great job explaining some of the demand underlying factors here on the strength. But U.S. was up about $5 billion or so sequentially.And I think there is a concern about whether U.S. can pick up the slack if there's regulations toward other geographies. And I was just wondering, as we go throughout the year, if this kind of surge in the U.S. continues and it's going to be -- whether that's OK.And if that underlies your growth rate, how can you keep growing so fast with this mix shift toward the U.S.? Your guidance looks like China is probably up sequentially. So, just wondering if you could go through that dynamic and maybe collect can weigh in. Thanks a lot.Jensen Huang -- President and Chief Executive OfficerChina is approximately the same percentage as Q4 and as previous quarters. It's about half of what it was before the export control. But it's approximately the same in percentage. With respect to geographies, the takeaway is that AI is software.It's modern software. It's incredible modern software, but it's modern software and AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quart of milk, it's delivered to you.AI was involved. And so almost everything that a consumer service provides AIs at the core of it. Every student will use AI as a tutor, healthcare services use AI, financial services use AI. No fintech company will not use AI.Every Fintech company will. Climate tech companies use AI. Mineral discovery now uses AI. The number of -- every higher education, every university uses AI and so I think it is fairly safe to say that AI has gone mainstream and that it's being integrated into every application.And our hope is that, of course, the technology continues to advance safely and advance in a helpful way to society. And with that, we're -- I do believe that we're at the beginning of this new transition. And what I mean by that in the beginning is, remember, behind us has been decades of data centers and decades of computers that have been built. And they've been built for a world of hand coding and general-purpose computing and CPUs and so on and so forth.And going forward, I think it's fairly safe to say that world is going to be almost all software to be infused with AI. All software and all services will be based on -- ultimately, based on machine learning, the data flywheel is going to be part of improving software and services and that the future computers will be accelerated, the future computers will be based on AI. And we're really two years into that journey. And in modernizing computers that have taken decades to build out.And so, I'm fairly sure that we're in the beginning of this new era. And then lastly, no technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has. And so, this is now a software tool that can address a much larger part of the world's GDP more than any time in history.And so, the way we think about growth, and the way we think about whether something is big or small has to be in the context of that. And when you take a step back and look at it from that perspective, we're really just in the beginning.OperatorYour next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Aaron, your line is open. Your next question comes from Mark Lipacis with Evercore ISI.Please go ahead.Mark Lipacis -- AnalystHi. It's Mark. Thanks for taking the question. I had a clarification and a question.Colette up for the clarification. Did you say that enterprise within the data center grew 2x year on year for the January quarter? And if so, does that -- would that make it the fast faster growing than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workflows being cloud services that enterprise is used. So, the question is, can you give us a sense of how that hyperscaler spend splits between that external workload and internal? And as these new AI workflows and applications come up, would you expect enterprises to become a larger part of that consumption mix? And does that impact how you develop your service, your ecosystem? Thank you.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentSure. Thanks for the question regarding our Enterprise business. Yes, it grew 2x and very similar to what we were seeing with our large CSPs. Keep in mind, these are both important areas to understand.Working with the CSPs can be working on large language models, can be working on inference in their own work. But keep in mind, that is also where the enterprises are servicing. Your enterprises are both with your CSPs as well as in terms of building on their own. They're both growing quite well.Jensen Huang -- President and Chief Executive OfficerThe CSPs are about half of our business. And the CSPs have internal consumption and external consumption, as you say. And we're using -- of course, used for internal consumption. We work very closely with all of them to optimize workloads that are internal to them because they have a large infrastructure of NVIDIA gear that they could take advantage of.And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, we're fungible. And so, the useful life of our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so, the second part is how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is, I believe, long term, it is by far larger and the reason for that is because if you look at the computer industry today and what is not served by the computer industry is largely industrial.So, let me give you an example. When we say enterprise, and let's use the car company as an example because they make both soft things and hard things. And so, in the case of a car company, the employees will be what we call enterprise and agentic AI and software planning systems and tools, and we have some really exciting things to share with you guys at GTC, build agentic systems are for employees to make employees more productive to design to market plan to operate their company. That's agentic AI.On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And today, there's 1 billion cars on the road. Someday, there will be 1 billion cars on the road, and every single one of those cars will be robotic cars, and they'll all be collecting data, and we'll be improving them using an AI factory.Whereas they have a car factory today, in the future, they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so, as you can see, there are three computers involved and there's the computer that helps the people. There's the computer that build the AI for the machineries that could be, of course, could be a tractor, it could be a lawn mower.It could be a human or robot that's being developed today. It could be a building. It could be a warehouse. These physical systems require new type of AI we call physical AI.They can't just understand the meaning of words and languages, but they have to understand the meaning of the world, friction and inertia, object permanence, and cause and effect. And all of those type of things that are common sense to you and I, but AIs have to go learn those physical effects. So, we call that physical AI. That whole part of using agentic AI to revolutionize the way we work inside companies, that's just starting.This is now the beginning of the agentic AI era, and you hear a lot of people talking about it, and we got some really great things going on. And then there's the physical AI after that, and then there are robotic systems after that. And so, these three computers are all brand new. And my sense is that long term, this will be by far the larger of a mall, which kind of makes sense.The world's GDP is representing -- represented by either heavy industries or industrials and companies that are providing for those.OperatorYour next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead.Aaron Rakers -- AnalystYeah. Thanks for letting me back in. Jensen, I'm curious as we now approach the two-year anniversary of really the Hopper inflection that you saw in 2023 in GenAI in general. And when we think about the road map you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective and whether if it's GB300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity? I'm just curious how you look at that.Jensen Huang -- President and Chief Executive OfficerYeah. I appreciate it. First of all, people are still using Voltas and Pascals, and Amperes. And the reason for that is because there are always things that because CUDA is so programmable you could use it Blackwell, one of the major use cases right now is data processing and data curation.You find a circumstance that an AI model is not very good at. You present that circumstance to a vision language model, let's say, it's a car. You present that circumstance to a vision language model. The vision language model actually looks in the circumstances, said, this is what happened and I was very good at it.You then take that response to the prompt, and you go and prompt an AI model to go find in your whole lake of data other circumstances like that, whatever that circumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the bottle. And so, you could use an peers to go and do data processing and data curation and machine learning-based search.And then you create the training data set, which you then present to your Hopper systems for training. And so, each one of these architectures are completely -- they're all CUDA-compatible and so everything wants on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the installed base of the past. All of us are very well employed.OperatorWe have time for one more question, and that question comes from Atif Malik with Citi. Please go ahead.Atif Malik -- AnalystHi. Thank you for taking my question. I have a follow-up question on gross margins for Colette. So, I understand there are many moving parts the Blackwell yields, NVLink 72, and Ethernet mix.And you kind of tipped to the earlier question the April quarter is the bottom, but second half would have to ramp like 200 basis points per quarter to get to the mid-70s range that you're giving for the end of the fiscal year. And we still don't know much about tariff impact to broader semiconductor, so what kind of gives you the confidence in that trajectory in the back half of this year?Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentYeah. Thanks for the question. Our gross margins, they're quite complex in terms of the material and everything that we put together in a Blackwell system, a tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well on Blackwell that will be able to help us do that.So, together, working after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we're going to probably start as soon as possible if we can. If we can improve it in the short term, we will also do that. Tariff at this point, it's a little bit of an unknown it's an unknown until we understand further what the U.S.government's plan is, both its timing, it's where, and how much. So, at this time, we are awaiting, but again, we would, of course, always follow export controls and/or tariffs in that manner.OperatorLadies and gentlemen, that does conclude our question-and-answer session. I'm sorry.Jensen Huang -- President and Chief Executive OfficerThank you.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentWe are going to open up to Jensen. A couple of things.Jensen Huang -- President and Chief Executive OfficerI just wanted to thank you. Thank you, Colette. The demand for Blackwell is extraordinary. AI is evolving beyond perception and generative AI into reasoning.With resenting AI, we're observing another scaling law, inference time or test time scaling, more computation. The more the model thinks the smarter the answer. Models like OpenAI, Grok 3, DeepSeek-R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100x more compute.Future reasoning models can consume much more compute. DeepSeek-R1 has ignited global enthusiasm. It's an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model.Nearly every AI developer is applying R1 or chain of thought and reinforcement learning techniques like R1 to scale their model's performance. We now have three scaling laws, as I mentioned earlier, driving the demand for AI computing. The traditional scaling loss of AI remains intact. Foundation models are being enhanced with multimodality, and pretraining is still growing.But it's no longer enough. We have two additional scaling dimensions: post-training scaling, where reinforcement learning, fine-tuning, model distillation require orders of magnitude more compute than pretraining alone; inference time scaling and reasoning where a single query and demand 100x more compute. We defined Blackwell for this moment, a single platform that can easily transition from pre-trading, post-training, and test time scaling. Blackwell's FP4 transformer engine and NVLink 72 scale-up fabric and new software technologies led Blackwell process reasoning AI models 25 times faster than Hopper.Blackwell in all of this configuration is in full production. Each Grace Blackwell NVLink 72 rack is an engineering marvel. 1.5 million components produced across 350 manufacturing sites by nearly 100,000 factory operators. AI is advancing at light speed.We're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI, multimodal AIs, enterprise AI sovereign AI and physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of capex to accelerated computing and AI.Data centers will increasingly become AI factories, and every company will have either rented or self-operated. I want to thank all of you for joining us today. Come join us at GTC in a couple of weeks. We're going to be talking about Blackwell Ultra, Rubin, and other new computing, networking, reasoning AI, physical AI products, and a whole bunch more.Thank you.Operator[Operator signoff]Duration: 0 minutesCall participants:Stewart Stecker -- Senior Director, Investor RelationsColette M. Kress -- Chief Financial Officer, Executive Vice PresidentC.J. Muse -- AnalystJensen Huang -- President and Chief Executive OfficerJoe Moore -- JPMorgan Chase and Company -- AnalystVivek Arya -- AnalystColette Kress -- Chief Financial Officer, Executive Vice PresidentHarlan Sur -- AnalystTimothy Arcuri -- AnalystBen Reitzes -- AnalystMark Lipacis -- AnalystAaron Rakers -- AnalystAtif Malik -- AnalystNVDA earnings call for the period ending September 30, 2024.IMAGE SOURCE: THE MOTLEY FOOL.Nvidia (NVDA 0.16%)Q3 2025 Earnings CallNov 20, 2024, 5:00 p.m. ETContents:* Prepared Remarks* Questions and Answers* Call ParticipantsPrepared Remarks:OperatorGood afternoon. My name is Jay, and I'll be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's third-quarter earnings call. All lines have been placed on mute to prevent any background noise.After the speakers' remarks, there will be a question-and-answer session. [Operator instructions] Thank you. Stewart Stecker, you may begin your conference.Stewart Stecker -- Senior Director, Investor RelationsThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2025. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations.These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 20, 2024, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentThank you, Stewart. Q3 was another record quarter. We continue to deliver incredible growth. Revenue of $35.1 billion was up 17% sequentially and up 94% year on year and well above our outlook of $32.5 billion.All market platforms posted strong sequential and year-over-year growth, fueled by the adoption of NVIDIA accelerated computing and AI. Starting with data center. Another record was achieved in data center. Revenue of $30.8 billion, up 17% sequential and up 112% year on year.NVIDIA Hopper demand is exceptional, and sequentially, NVIDIA H200 sales increased significantly to double-digit billions, the fastest prod ramp in our company's history. The H200 delivers up to 2x faster inference performance and up to 50% improved TCO. Cloud service providers were approximately half of our data center sales with revenue increasing more than 2x year on year. CSPs deployed NVIDIA H200 infrastructure and high-speed networking with installations scaling to tens of thousands of DPUs to grow their business and serve rapidly rising demand for AI training and inference workloads.NVIDIA H200-powered cloud instances are now available from AWS, CoreWeave, and Microsoft Azure, with Google Cloud and OCI coming soon. Alongside significant growth from our large CSPs, NVIDIA GPU regional cloud revenue jumped year on year as North America, India, and Asia Pacific regions ramped NVIDIA Cloud instances and sovereign cloud build-outs. Consumer Internet revenue more than doubled year on year as companies scaled their NVIDIA Hopper infrastructure to support next-generation AI models training, multimodal, and agentic AI, deep learning recommender engines, and generative AI inference and content creation workloads. NVIDIA Ampere and Hopper infrastructures are fueling inference revenue growth for customers.NVIDIA is the largest inference platform in the world. Our large installed base and rich software ecosystem encourage developers to optimize for NVIDIA and deliver continued performance and TCO improvements. Rapid advancements in NVIDIA software algorithms boosted Hopper inference throughput by an incredible 5x in one year and cut time to first token by 5x. Our upcoming release of NVIDIA NIM will boost Hopper inference performance by an additional 2.4x.Continuous performance optimizations are a hallmark of NVIDIA and drive increasingly economic returns for the entire NVIDIA installed base. Blackwell is in full production after a successfully executed change. We shipped 13,000 GPU samples to customers in the third quarter, including one of the first Blackwell DGX engineering samples to OpenAI. Blackwell is a full-stack, full-infrastructure, AI data center scale system with customizable configurations needed to address a diverse and growing AI market from x86 to ARM, training to inferencing GPUs, InfiniBand to Ethernet switches, and NVLink.And from liquid-cooled to air-cooled, every customer is racing to be the first to market. Blackwell is now in the hands of all of our major partners, and they are working to bring up their data centers. We are integrating Blackwell systems into the diverse data center configurations of our customers. Blackwell demand is staggering, and we are racing to scale supply to meet the incredible demand customers are placing on us.Customers are gearing up to deploy Blackwell at scale. Oracle announced the world's first Zettascale AI cloud computing clusters that can scale to over 131,000 Blackwell GPUs to help enterprises train and deploy some of the most demanding next-generation AI models. Yesterday, Microsoft announced they will be the first CSP to offer, in private preview, Blackwell-based cloud instances powered by NVIDIA GB200 and Quantum InfiniBand. Last week, Blackwell made its debut on the most recent round of MLPerf training results, sweeping the per GPU benchmarks and delivering a 2.2x leap in performance over Hopper.The results also demonstrate our relentless pursuit to drive down the cost of compute. The 64 Blackwell GPUs are required to run the GPT-3 benchmark compared to 256 H100s or a 4x reduction in cost. NVIDIA Blackwell architecture with NVLink Switch enables up to 30x faster inference performance and a new level of inference scaling, throughput, and response time that is excellent for running new reasoning inference applications like OpenAI's o1 model. With every new platform shift, a wave of start-ups is created.Hundreds of AI-native companies are already delivering AI services with great success. Though Google, Meta, Microsoft, and OpenAI are the headliners, Anthropic, Perplexity, Mistral, Adobe Firefly, Runway, Midjourney, Light Tricks, Harvey, Podium, Purser, and the Bridge are seeing great success while thousands of AI-native start-ups are building new services. The next wave of AI are Enterprise AI and industrial AI. Enterprise AI is in full throttle.NVIDIA AI Enterprise, which includes NVIDIA NeMo and NIMs micro services is an operating platform of agentic AI. Industry leaders are using NVIDIA AI to build Copilots and agents. Working with NVIDIA, Cadence, Cloudera, Cohesity, NetApp, Salesforce, SAP, and ServiceNow are racing to accelerate development of these applications with the potential for billions of agents to be deployed in the coming years. Consulting leaders like Accenture and Deloitte are taking NVIDIA AI to the world's enterprises.Accenture launched a new business group with 30,000 professionals trained on NVIDIA AI technology to help facilitate this global build-out. Additionally, Accenture, with over 707,000 employees, is leveraging NVIDIA-powered agentic AI applications internally, including one case that cuts manual steps in marketing campaigns for 25% to 35%. Nearly 1,000 companies are using NVIDIA NIM, and the speed of its uptake is evident in NVIDIA AI Enterprise monetization. We expect NVIDIA AI Enterprise full-year revenue to increase over 2x from last year, and our pipeline continues to build.Overall, our software, service, and support revenue is annualizing at $1.5 billion, and we expect to exit this year annualizing at over $2 billion. Industrial AI and robotics are accelerating. This is triggered by breakthroughs in physical AI, foundation models that understand the physical world, like NVIDIA NeMo for enterprise AI agents. We built NVIDIA Omniverse for developers to build, train, and operate industrial AI and robotics.Some of the largest industrial manufacturers in the world are adopting NVIDIA Omniverse to accelerate their businesses, automate their workflows, and to achieve new levels of operating efficiency. Foxconn, the world's largest electronics manufacturer, is using digital twin and industrial AI built on NVIDIA Omniverse to speed the bring-up of its Blackwell factories and drive new levels of efficiency. In its Mexico facility alone, Foxconn expects to reduce -- a reduction of over 30% in annual hour usage. From a geographic perspective, our data center revenue in China grew sequentially due to shipments of export-compliant Hopper products to industries.As a percentage of total data center revenue, it remained well below levels prior to the onset of export controls. We expect the market in China to remain very competitive going forward. We will continue to comply with export controls while serving our customers. Our AI initiatives continue to gather momentum as countries embrace NVIDIA accelerated computing for a new industrial revolution powered by AI.India's leading CSPs include product communications and data services are building AI factories for tens of thousands of NVIDIA GPUs. By year-end, they will have boosted NVIDIA GPU deployments in the country by nearly 10x. Infosys, TFC, Wipro are adopting NVIDIA AI Enterprise and upskilling nearly 0.5 million developers and consultants to help clients build and run AI agents on our platform. In Japan, SoftBank is building the nation's most powerful AI supercomputer with NVIDIA DGX Blackwell and Quantum InfiniBand.SoftBank is also partnering with NVIDIA to transform the telecommunications network into a distributed AI network with NVIDIA AI Aerial and ARN platform that can process both 5G RAN on AI on CUDA. We are launching the same in the U.S. with T-Mobile. Leaders across Japan, including Fujitsu, NEC, and NTT are adopting NVIDIA AI Enterprise, and major consulting companies, including EY Strategy and Consulting will help bring NVIDIA AI technology to Japan's industries.Networking revenue increased 20% year on year. Areas of sequential revenue growth include InfiniBand and Ethernet switches, SmartNICs, and BlueField DPUs. Though networking revenue was sequentially down, networking demand is strong and growing, and we anticipate sequential growth in Q4. CSPs and supercomputing centers are using and adopting the NVIDIA InfiniBand platform to power new H200 clusters.NVIDIA Spectrum-X Ethernet for AI revenue increased over 3x year on year. And our pipeline continues to build with multiple CSPs and consumer Internet companies planning large cluster deployments. Traditional Ethernet was not designed for AI. NVIDIA Spectrum-X uniquely leverages technology previously exclusive to InfiniBand to enable customers to achieve massive scale of their compute.Utilizing Spectrum-X, xAI's Colossus 100,000-Hopper supercomputer experienced zero application latency degradation and maintained 95% of data throughput versus 60% for traditional Ethernet. Now, moving to Gaming and AI PCs. Gaming revenue of $3.3 billion increased 14% sequentially and 15% year on year. Q3 was a great quarter for Gaming with notebook, console, and desktop revenue all growing sequentially and year on year.RTX end demand was fueled by strong back-to-school sales as consumers continue to choose GeForce RTX GPUs and devices to power gaming, creative, and AI applications. Channel inventory remains healthy, and we are gearing up for the holiday season. We began shipping new GeForce RTX AI PC with up to 321 AI TOPS from ASUS and MSI with Microsoft's Copilot+ capabilities anticipated in Q4. These machines harness the power of RTX ray tracing and AI technologies to supercharge gaming, photo, and video editing, image generation, and coding.This past quarter, we celebrated the 25th anniversary of the GeForce 256, the world's first GPU. The transforming executing graphics to igniting the AI revolution. NVIDIA's GPUs have been the driving force behind some of the most consequential technologies of our time. Moving to ProViz.Revenue of $486 million was up 7% sequentially and 17% year on year. NVIDIA RTX workstations continue to be the preferred choice to power professional graphics, design, and engineering-related workloads. Additionally, AI is emerging as a powerful demand driver, including autonomous vehicle simulation, generative AI model prototyping for productivity-related use cases, and generative AI content creation in media and entertainment. Moving to Automotive.Revenue was a record $449 million, up 30% sequentially and up 72% year on year. Strong growth was driven by self-driving brands of NVIDIA Orin and robust end market demand for NAVs. Global Cars is rolling out its fully electric SUV built on NVIDIA Orin and DriveOS. OK, moving to the rest of the P&L.GAAP gross margin was 74.6% and non-GAAP gross margin was 75%, down sequentially, primarily driven by a mix shift of the H100 systems to more complex and higher-cost systems within data center. Sequentially, GAAP operating expenses and non-GAAP operating expenses were up 9% due to higher compute, infrastructure, and engineering development costs for new product introductions. In Q3, we returned $11.2 billion to shareholders in the form of share repurchases and cash dividends. Well, let me turn to the outlook for the fourth quarter.Total revenue is expected to be $37.5 billion, plus or minus 2%, which incorporates continued demand for Hopper architecture and the initial ramp of our Blackwell products. While demand greatly exceed supply, we are on track to exceed our previous Blackwell revenue estimate of several billion dollars as our visibility into supply continues to increase. On Gaming, although sell-through was strong in Q3, we expect fourth-quarter revenue to decline sequentially due to supply constraints. GAAP and non-GAAP gross margins are expected to be 73% and 73.5%, respectively, plus or minus 50 basis points.Blackwell is a customizable AI infrastructure with seven different types of NVIDIA-built chips, multiple networking options, and for air and liquid-cooled data centers. Our current focus is on ramping to strong demand, increasing system availability, and providing the optimal mix of configurations to our customer. As Blackwell ramps, we expect gross margins to moderate to the low 70s. When fully ramp, we expect Blackwell margins to be in the mid-70s.GAAP and non-GAAP operating expenses are expected to be approximately $4.8 billion and $3.4 billion, respectively. We are a data center-scale AI infrastructure company. Our investments include building data centers for development of our hardware and software stacks and to support new introductions. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $400 million, excluding gains and losses from nonaffiliated investments.GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We will be attending the UBS Global Technology and AI Conference on December 3 in Scottsdale.Please join us at CES in Las Vegas, where Jensen will deliver a keynote on January 6, and we will host a Q&A session for financial analysts the next day on January 7. Our earnings call to discuss results for the fourth quarter of fiscal 2025 is scheduled for February 26, 2025. We will now open the call for questions. Operator, can you poll for questions, please?Questions & Answers:Operator[Operator instructions] We'll pause for just a moment to compile the Q&A roster. As a reminder, please limit yourself to one question. Your first question comes from the line of C.J. Muse of Cantor Fitzgerald.Your line is open.C.J. Muse -- AnalystYeah. Good afternoon. Thank you for taking the question. I guess just a question for you on the debate around whether scaling for large language models have stalled.Obviously, we're very early here, but would love to hear your thoughts on this front. How are you helping your customers as they work through these issues? And then obviously, part of the context here is we're discussing clusters that have yet to benefit from Blackwell. So, is this driving even greater demand for Blackwell? Thank you.Jensen Huang -- President and Chief Executive OfficerOur foundation model pretraining scaling is intact, and it's continuing. As you know, this is an empirical law, not a fundamental physical law. But the evidence is that it continues to scale. What we're learning, however, is that it's not enough, that we've now discovered two other ways to scale.One is post-training scaling. Of course, the first generation of post-training was reinforcement learning human feedback, but now we have reinforcement learning AI feedback, and all forms of synthetic data generated data that assists in post-training scaling. And one of the biggest events and one of the most exciting developments is Strawberry, ChatGPT o1, OpenAI's o1, which does inference time scaling, what is called test time scaling. The longer it thinks, the better and higher-quality answer it produces.And it considers approaches like chain of thought and multi-path planning and all kinds of techniques necessary to reflect and so on and so forth. And it's -- intuitively, it's a little bit like us doing thinking in our head before we answer your question. And so, we now have three ways of scaling, and we're seeing all three ways of scaling. And as a result of that, the demand for our infrastructure is really great.You see now that at the tail end of the last generation of foundation models were at about 100,000 Hoppers. The next generation starts at 100,000 Blackwells. And so, that kind of gives you a sense of where the industry is moving with respect to pretraining scaling, post-training scaling, and then now very importantly, inference time scaling. And so, the demand is really great for all of those reasons.But remember, simultaneously, we're seeing inference really starting to scale up for our company. We are the largest inference platform in the world today because our installed base is so large. And everything that was trained on Amperes and Hoppers inference incredibly on Amperes and Hoppers. And as we move to Blackwells for training foundation models, it leads behind it a large installed base of extraordinary infrastructure for inference.And so, we're seeing inference demand go up. We're seeing inference time scaling go up. We see the number of AI native companies continue to grow. And of course, we're starting to see enterprise adoption of agentic AI really is the latest rage.And so, we're seeing a lot of demand coming from a lot of different places.OperatorYour next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.Toshiya Hari -- AnalystHi. Good afternoon. Thank you so much for taking the question. Jensen, you executed the mass change earlier this year.There were some reports over the weekend about some heating issues. On the back of this, we've had investors ask about your ability to execute to the road map you presented at GTC this year with Ultra coming out next year and the transition to Rubin in '26. Can you sort of speak to that? And some investors are questioning that, so if you can sort of speak to your ability to execute on time, that would be super helpful. And then a quick part B.On supply constraints, is it a multitude of componentry that's causing this, or is it specifically HBN? Is it supply constraints? Are the supply constraints getting better? Are they worsening? Any sort of color on that would be super helpful as well. Thank you.Jensen Huang -- President and Chief Executive OfficerYeah. Thanks. So, let's see. Back to the first question.Blackwell production is in full steam. In fact, as Colette mentioned earlier, we will deliver this quarter more Blackwells than we had previously estimated. And so, the supply chain team is doing an incredible job working with our supply partners to increase Blackwell, and we're going to continue to work hard to increase Blackwell through next year. It is the case that demand exceeds our supply.And that's expected as we're in the beginnings of this generative AI revolution as we all know. And we're at the beginning of a new generation of foundation models that are able to do reasoning and able to do long thinking. And of course, one of the really exciting areas is physical AI, AI that now understands the structure of the physical world. And so, Blackwell demand is very strong.Our execution is going well. And there's obviously a lot of engineering that we're doing across the world. You see now systems that are being stood up by Dell and CoreWeave. I think you saw systems from Oracle stood up.You have systems from Microsoft, and they're about to preview their Grace Blackwell systems. You have systems that are at Google. And so, all of these CSPs are racing to be first. The engineering that we do with them is, as you know, rather complicated.And the reason for that is because, although we build full stack and full infrastructure, we disaggregate all of this AI supercomputer, and we integrate it into all of the custom data centers and architectures around the world. That integration process, it's something we've done several generations now. We're very good at it but still there's still a lot of engineering that happens at this point. But as you see from all of the systems that are being stood up, Blackwell is in great shape.And as we mentioned earlier, the supply and what we're planning to ship this quarter is greater than our previous estimates. With respect to the supply chain, there are seven different chips, seven custom chips that we built in order for us to deliver the Blackwell systems. The Blackwell systems go in air-cooled or liquid-cooled, NVLink 8 or NVLink 72 or NVLink 8, NVLink 36, NVLink 72. We have x86 or Grace.And the integration of all of those systems into the world's data centers is nothing short of a miracle. And so, the component supply chain necessary to ramp at this scale, you have to go back and take a look at how much Blackwell we shipped last quarter, which was zero. And in terms of how much Blackwell total systems will ship this quarter, which is measured in billions, the ramp is incredible. And so almost every company in the world seems to be involved in our supply chain.And we've got great partners, everybody from, of course, TSMC and Amphenol, the connector company, incredible company; Vertiv and SK Hynix and Micron; Spill Amcor; KYEC; and there's Foxconn and the factories that they've built; and Quanta and Wiwynn; and, gosh, Dell and HP, and Super Micro, Lenovo. And the number of companies is just really quite incredible. Quanta. And I'm sure I've missed partners that are involved in the ramping of Blackwell, which I really appreciate.And so, anyways, I think we're in great shape with respect to the Blackwell ramp at this point. And then lastly, your question about our execution of our road map. We're on an annual road map and we're expecting to continue to execute on our annual road map. And by doing so, we increased the performance, of course, of our platform.But it's also really important to realize that when we're able to increase performance and do so at factors at a time, we're reducing the cost of training. We're reducing the cost of inferencing. We're reducing the cost of AI so that it could be much more accessible. But the other factor that's very important to note is that when there's a data center of some fixed size and the data center always is of some fixed size.It could be, of course, tens of megawatts in the past, and now it's -- most data centers are now 100 megawatts to several hundred megawatts, and we're planning on gigawatt data centers, it doesn't really matter how large the data centers are. The power is limited. And when you're in the power-limited data center, the best -- the highest performance per watt translates directly into the highest revenues for our partners. And so, on the one hand, our annual road map reduces costs.But on the other hand, because our perf per watt is so good compared to anything out there, we generate for our customers the greatest possible revenues. And so, that annual rhythm is really important to us, and we have every intention of continuing to do that. And everything is on track as far as I know.OperatorYour next question comes from the line of Timothy Arcuri of UBS. Your line is open.Timothy Arcuri -- AnalystThanks a lot. I'm wondering if you can talk about the trajectory of how Blackwell is going to ramp this year. I know Jensen, you did just talk about Blackwell being better than -- I think you had said several billions of dollars in January. It sounds like you're going to do more than that.But I think in recent months also, you said that Blackwell crosses over Hopper in the April quarter. So, I guess I had two questions. First of all, is that still the right way to think about it, that Blackwell will cross over Hopper in April? And then Colette, you kind of talked about Blackwell bringing down gross margin to the low 70s as it ramps. So, I guess if April is the crossover, is that the worst of the pressure on gross margin? So, you're going to be kind of in the low 70s as soon as April.I'm just wondering if you can sort of shape that for us. Thanks.Jensen Huang -- President and Chief Executive OfficerColette, why don't you start?Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentSure. Let me first start with your question, Tim, thank you, regarding our gross margins. And we discussed that our gross margins, as we are ramping Blackwell in the very beginning, and the many different configurations, the many different chips that we are bringing to market, we are going to focus on making sure we have the best experience for our customers as they stand that up. We will start growing into our gross margins, but we do believe those will be in the low 70s in that first program.So, you're correct, as you look at the quarters following after that, we will start increasing our gross margins, and we hope to get to the mid-70s quite quickly as part of that round.Jensen Huang -- President and Chief Executive OfficerHopper demand will continue through next year, surely the first several quarters of the next year. And meanwhile, we will ship more Blackwells next quarter than this, and we'll ship more Blackwells the quarter after that than our first quarter. And so, that kind of puts it in perspective. We are really at the beginnings of two fundamental shifts in computing that is really quite significant.The first is moving from coding that runs on CPUs to machine learning that creates neural networks that runs on GPUs. And that fundamental shift from coding to machine learning is widespread at this point. There are no companies who are not going to do machine learning. And so, machine learning is also what enables generative AI.And so, on the one hand, the first thing that's happening is $1 trillion worth of computing systems and data centers around the world is now being modernized for machine learning. On the other hand, secondarily, I guess, is that on top of these systems are going to be -- we're going to be creating a new type of capability called AI. And when we say generative AI, we're essentially saying that these data centers are really AI factories. They're generating something.Just like we generate electricity, we're now going to be generating AI. And if the number of customers is large, just as the number of consumers of electricity is large, these generators are going to be running 24/7. And today, many AI services are running 24/7, just like an AI factory. And so, we're going to see this new type of system come online, and I call it an AI factory because that's really as close to what it is.It's unlike a data center of the past. And so, these two fundamental trends are really just beginning. And so, we expect this to happen, this growth, this modernization, and the creation of a new industry to go on for several years.OperatorYour next question comes from the line of Vivek Arya of Bank of America Securities. Your line is open.Vivek Arya -- AnalystThanks for taking my question. Colette, just to clarify, do you think it's a fair assumption to think NVIDIA could recover to kind of mid-70s gross margin in the back half of calendar '25? Just wanted to clarify that. And then Jensen, my main question, historically, when we have seen hardware deployment cycles, they have inevitably included some digestion along the way. When do you think we get to that phase? Or is it just too premature to discuss that because you're just at the start of Blackwell? So, how many quarters of shipments do you think is required to kind of satisfy this first wave? Can you continue to grow this into calendar '26? Just how should we be prepared to see what we have seen historically, right, a period of digestion along the way of a long-term kind of secular hardware deployment?Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentOK. Vivek, thank you for the question. Let me clarify your question regarding gross margins. Could we reach the mid-70s in the second half of next year? And yes, I think it is a reasonable assumption or goal for us to do, but we'll just have to see how that mix of ramp goes.But yes, it is definitely possible.Jensen Huang -- President and Chief Executive OfficerThe way to think through that, Vivek, is I believe that there will be no digestion until we modernize $1 trillion with the data centers. Those -- if you just look at the world's data centers, the vast majority of it is built for a time when we wrote applications by hand and we ran them on CPUs. It's just not a sensible thing to do anymore. If you have -- if every company's capex -- if they're ready to build data center tomorrow, they ought to build it for a future of machine learning and generative AI because they have plenty of old data centers.And so, what's going to happen over the course of the next X number of years, and let's assume that over the course of four years, the world's data centers could be modernized as we grow into IT, as you know, IT continues to grow about 20%, 30% a year, let's say. But let's say by 2030, the world's data centers for computing is, call it, a couple of trillion dollars. We have to grow into that. We have to modernize the data center from coding to machine learning.That's number one. The second part of it is generative AI. And we're now producing a new type of capability the world's never known, a new market segment that the world's never had. If you look at OpenAI, it didn't replace anything.It's something that's completely brand new. It's, in a lot of ways as when the iPhone came, was completely brand new. It wasn't really replacing anything. And so, we're going to see more and more companies like that.And they're going to create and generate, out of their services, essentially intelligence. Some of it would be digital artist intelligence like Runway. Some of it would be basic intelligence like OpenAI. Some of it would be legal intelligence like Harvey, digital marketing intelligence like Rider's, so on and so forth.And the number of these companies, these -- what are they called, AI-native companies, are just in hundreds. And almost every platform shift, there was -- there were Internet companies, as you recall. There were cloud-first companies. There were mobile-first companies.Now, they're AI natives. And so, these companies are being created because people see that there's a platform shift, and there's a brand-new opportunity to do something completely new. And so, my sense is that we're going to continue to build out to modernize IT, modernize computing, number one; and then number two, create these AI factories that are going to be for a new industry for the production of artificial intelligence.OperatorYour next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.Stacy Rasgon -- AnalystHi, guys. Thanks for taking my questions. Colette, I had a clarification and a question for you. The clarification, just when you say low 70s gross margins, does 73.5% count as low 70s, or do you have something else in mind? And for my question, you're guiding total revenues, and so I mean, total data center revenues in the next quarter must be up several billion dollars.But it sounds like Blackwell now should be up more than that. But you also said Hopper was still strong, so like is Hopper down sequentially next quarter? And if it is, like why? Is it because of the supply constraints? China has been pretty strong. Is China is kind of rolling off a bit into Q4? So any color you can give us on sort of the Blackwell ramp and the Blackwell versus Hopper behavior into Q4 would be really helpful. Thank you.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentSo, first, starting on your first question there, Stacy, regarding our gross margin and define low. Low, of course, is below the mids, and let's say we might be at 71%, maybe about 72%, 72.5%. We're going to be in that range. We could be higher than that as well.We're just going to have to see how it comes through. We do want to make sure that we are ramping and continuing that improvement, the improvement in terms of our yields, the improvement in terms of the product as we go through the rest of the year, so we'll get up to the mid-70s by that point. The second statement was a question regarding our Hopper and what is our Hopper doing. We have seen substantial growth for H200 not only in terms of orders but the quickness in terms of those that are standing that out.It is an amazing product and it's the fastest growing and ramping that we've seen. We will continue to be selling Hopper in this quarter, in Q4 for sure. That is across the board in terms of all of our different configurations, and our configurations include what we may do in terms of China. But keep that in mind that folks are also, at the same time, looking to build out their Blackwell.So, we've got a little bit of both happening in Q4. But yes. Is it possible for Hopper to grow between Q3 and Q4? It's possible, but we'll just have to see.OperatorYour next question comes from the line of Joseph Moore of Morgan Stanley. Your line is open.Joseph Moore -- AnalystGreat. Thank you. I wonder if you could talk a little bit about what you're seeing in the inference market. You've talked about Strawberry and some of the ramifications of longer scaling influence projects.But you've also talked about the possibility that as some of these Hopper clusters age, that you could use some of the Hopper chips for inference. So, I guess do you expect inference to outgrow training in the next kind of 12 months time frame? And just generally, your thoughts there.Jensen Huang -- President and Chief Executive OfficerOur hopes and dreams is that someday, the world does a ton of inference. And that's when AI has really exceeded is when every single company is doing inference inside their companies for the marketing department and forecasting department and supply chain group and their legal department and engineering, of course, and coding of course. And so, we hope that every company is doing inference 24/7 and that there will be a whole bunch of AI native start-ups, thousands of AI native start-ups that are generating tokens and generating AI. And every aspect of your computer experience from using Outlook to PowerPointing or when you're sitting there with Excel, you're constantly generating tokens.And every time you read a PDF, open a PDF, it generated a whole bunch of tokens. One of my favorite applications is NotebookLM, this Google application that came out. I used the living daylights out of it just because it's fun. And I put every PDF, every archived paper into it just to listen to it as well as scanning through it.And so, I think that's the goal is to train these models so that people use it. And there's now a whole new era of AI, if you will, a whole new genre of AI called physical AI. Just those large language models understand the human language and how the thinking process, if you will. Physical AI understands the physical world.And it understands the meaning of the structure and understands what's sensible and what's not and what could happen and what won't. And not only does it understand but it can predict, roll out a short future. That capability is incredibly valuable for industrial AI and robotics. And so, that's fired up so many AI-native companies and robotics companies and physical AI companies that you're probably hearing about.And it's really the reason why we built Omniverse. Omniverse is -- so that we can enable these AIs to be created and learn in Omniverse and learn from synthetic data generation and reinforcement, learning physics feedback. Instead of just a human feedback, it's now physics feedback. To have these capabilities, Omniverse was created so that we can enable physical AI.And so, that -- the goal is to generate tokens. The goal is to inference, and we're starting to see that growth happening. So, I'm super excited about that. Now, let me just say one more thing.Inference is super hard. And the reason why inference is super hard is because you need the accuracy to be high on the one hand. You need the throughput to be high so that the cost could be as low as possible, but you also need the latency to be low. And computers that are high-throughput as well as low latency is incredibly hard to build.And these applications have long context lengths because they want to understand. They want to be able to inference within understanding the context of what they're being asked to do. And so, the context length is growing larger and larger. On the other hand, the models are getting larger.They're multimodality. Just the number of dimensions that inference is innovating is incredible. And this innovation rate is what makes NVIDIA's architecture so great because we -- our ecosystem is fantastic. Everybody knows that if they innovate on top of CUDA and top of NVIDIA's architecture, they can innovate more quickly and they know that everything should work.And if something were to happen, it's probably likely their code and not ours. And so, that ability to innovate in every single direction at the same time, having a large installed base so that whatever you create could land on an NVIDIA computer and be deployed broadly all around the world in every single data center all the way out to the edge into robotic systems, that capability is really quite phenomenal.OperatorYour next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.Aaron Rakers -- AnalystYeah. Thanks for taking the question. I wanted to ask you as we kind of focus on the Blackwell cycle and think about the data center business. When I look at the results this last quarter, Colette, you mentioned that obviously, the networking business was down about 15% sequentially.But then your comments were that you were seeing very strong demand. You mentioned also that you had multiple cloud CFP design wins for these large-scale clusters. So, I'm curious if you could unpack what's going on in the Networking business and where maybe you've seen some constraints and just your confidence in the pace of Spectrum-X progressing to that multiple billions of dollars that you previously had talked about. Thank you.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentLet's first start with the networking. The growth year over year is tremendous. And our focus since the beginning of our acquisition of Mellanox has really been about building together the work that we do in terms of in the data center. The networking is such a critical part of that.Our ability to sell our networking with many of our systems that we are doing in data center is continuing to grow and do quite well. So, this quarter is just a slight dip down and we're going to be right back up in terms of growing. We're getting ready for Blackwell and more and more systems that will be using not only our existing networking but also the networking that is going to be incorporated in a lot of these large systems we are providing them to.OperatorYour next question comes from the line of Atif Malik of Citi. Your line is open.Atif Malik -- AnalystThank you for taking my question. I have two quick ones for Colette. Colette, on the last earnings call, you mentioned that sovereign demand is in low double-digit billions. Can you provide an update on that? And then can you explain the supply constrained situation in gaming? Is that because you're shifting your supply toward data center?Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentSo, first, starting in terms of sovereign AI, such an important part of growth, something that is really surfaced with the onset of generative AI and building models in the individual countries around the world. And we see a lot of them, and we talked about a lot of them on the call today and the work that they are doing. So, our sovereign AI and our pipeline going forward is still absolutely intact as those are working to build these foundational models in their own language, in their own culture, and working in terms of the enterprises within those countries. And I think you'll continue to see this be growth opportunities that you may see with our regional clouds that are being stood up and/or those that are focusing in terms of AI factories for many parts of the sovereign AI.This is areas where this is growing not only in terms of in Europe, but you're also seeing this in terms of growth in terms of in the Asia Pac as well. Let me flip to your second question that you asked regarding Gaming. So, our Gaming right now from a supply, we're busy trying to make sure that we can ramp all of our different products. And in this case, our gaming supply, given what we saw selling through was moving quite fast.Now, the challenge that we have is how fast could we get that supply getting ready into the market for this quarter. Not to worry, I think we'll be back on track with more supply as we turn the corner into the new calendar year. We're just going to be tight for this quarter.OperatorYour next question comes from the line of Ben Reitzes of Melius Research. Your line is open.Ben Reitzes -- Melius Research -- AnalystYeah. Hi. Thanks a lot for the question. I wanted to ask Colette and Jensen with regard to sequential growth.So, very strong sequential growth this quarter, and you're guiding to about 7%. Do your comments on Blackwell imply that we reaccelerate from there as you get more supply just in the first half, there would be some catch-up. So, I was wondering how prescriptive you could be there. And then, Jensen, just overall with the change in administration that's going to take place here in the U.S.and the China situation, have you gotten any sense or any conversations about tariffs or anything with regard to your China business? Any sense of what may or may not go on? It's probably too early, but wondering if you had any thoughts there. Thanks so much.Jensen Huang -- President and Chief Executive OfficerWe guide one quarter at a time.Colette M. Kress -- Chief Financial Officer, Executive Vice PresidentWe are working right now on the quarter that we're in and building what we need to ship in terms of Blackwell. We have every supplier on the planet working seamlessly with us to that. And once we get to next quarter, we'll help you understand in terms of that ramp that we'll see to the next quarter going after that.Jensen Huang -- President and Chief Executive OfficerWhatever the new administration decides, we'll, of course, support the administration. And that's our -- the highest mandate. And then after that, do the best we can and just as we always do. And so, we have to simultaneously and we will comply with any regulation that comes along fully and support our customers to the best of our abilities and to compete in the marketplace.We'll do all of these three things simultaneously.OperatorYour final question comes from the line of Pierre Ferragu of New Street Research. Your line is open.Pierre Ferragu -- AnalystHey, thanks for taking my question. Jensen, you mentioned in your comments, you have the pretraining, the actual language models and you have reinforcement learning that becomes more and more important in training and inference as well and then you have inference itself. And I was wondering if you have a sense like a high-level typical sense of out of an overall AI ecosystem, like maybe one of your clients or one of the large models that are out there today, how much of the compute goes into each of these buckets? How much for the pretraining? How much for the reinforcement? And how much into inference today? Do you have any sense for how it's splitting and where the growth is the most important as well?Jensen Huang -- President and Chief Executive OfficerWell, today, it's vastly in pretraining a foundation model because, as you know, post-training, the new technologies are just coming online. And whatever you could do in pretraining and post-training, you would try to do so that the inference cost could be as low as possible for everyone. However, there are only so many things that you could do a priority. And so, you'll always have to do on-the-spot thinking and in-context thinking and reflection.And so, I think that the fact that all three are scaling is actually very sensible based on where we are. And in the area foundation model, now we have multimodality foundation models, and the amount of petabytes video that these foundation models are going to be trained on, it's incredible. And so, my expectation is that for the foreseeable future, we're going to be scaling pretraining, post-training, as well as inference time scaling and which is the reason why I think we're going to need more and more compute. And we're going to have to drive as hard as we can to keep increasing the performance by X factors out of time so that we can continue to drive down the cost and continue to increase the revenues and get the AI revolution going.OperatorThank you. I'd like to turn the call back over to Jensen Huang for closing remarks.Jensen Huang -- President and Chief Executive OfficerThank you. The tremendous growth in our business is being fueled by two fundamental trends that are driving global adoption of NVIDIA computing. First, the computing stack is undergoing a reinvention, a platform shift from coding to machine learning, from executing code on CPUs to processing neural networks on GPUs. The $1 trillion installed base of traditional data center infrastructure is being rebuilt for Software 2.0, which applies machine learning to produce AI.Second, the age of AI is in full steam. Generative AI is not just a new software capability but a new industry with AI factories manufacturing digital intelligence, a new industrial revolution that can create a multi-trillion-dollar AI industry. Demand for Hopper and anticipation for Blackwell, which is now in full production, are incredible for several reasons. There are more foundation model makers now than there were a year ago.The computing scale of pretraining and post-training continues to grow exponentially. There are more AI-native start-ups than ever, and the number of successful inference services is rising. And with the introduction of ChatGPT o1, OpenAI o1, a new scaling law called test time scaling has emerged. All of these consume a great deal of computing.AI is transforming every industry, company, and country. Enterprises are adopting agentic AI to revolutionize workflows. Over time, AI coworkers will assist employees in performing their jobs faster and better. Investments in industrial robotics are surging due to breakthroughs in physical AI, driving new training infrastructure demand as researchers train world foundation models on petabytes of video and Omniverse synthetically generated data.The age of robotics is coming. Countries across the world recognize the fundamental AI trends we are seeing and have awakened to the importance of developing their national AI infrastructure. The age of AI is upon us, and it's large and diverse. NVIDIA's expertise, scale, and ability to deliver full stack and full infrastructure lets us serve the entire multitrillion-dollar AI and robotics opportunities ahead from every hyperscale cloud, enterprise private cloud to sovereign regional AI clouds, on-prem to industrial edge and robotics.Thanks for joining us today, and catch up next time.Operator[Operator signoff]Duration: 0 minutesCall participants:Stewart Stecker -- Senior Director, Investor RelationsColette M. Kress -- Chief Financial Officer, Executive Vice PresidentC.J. Muse -- AnalystJensen Huang -- President and Chief Executive OfficerToshiya Hari -- AnalystTimothy Arcuri -- AnalystColette Kress -- Chief Financial Officer, Executive Vice PresidentVivek Arya -- AnalystStacy Rasgon -- AnalystJoseph Moore -- AnalystAaron Rakers -- AnalystAtif Malik -- AnalystBen Reitzes -- Melius Research -- Analyst